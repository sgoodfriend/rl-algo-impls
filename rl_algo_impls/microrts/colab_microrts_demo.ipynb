{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgoodfriend/rl-algo-impls/blob/cog2024/rl_algo_impls/microrts/colab_microrts_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NTyZ8p0HR3TC"
      },
      "source": [
        "# Running Python MicroRTS submission in Google Colab\n",
        "[sgoodfriend/rl-algo-impls](https://github.com/sgoodfriend/rl-algo-impls) was used to [train a Python agent to play MicroRTS](https://wandb.ai/sgoodfriend/rl-algo-impls-benchmarks/reports/rl-algo-impls-MicroRTS-Training--Vmlldzo0NjA2NTAy). [Farama-Foundation/MicroRTS-Py](https://github.com/Farama-Foundation/MicroRTS-Py) was the inspiration for this training.\n",
        "\n",
        "Given MicroRTS is a Java environment, a Python server runs the Pytorch model, which communicates over a socket connection. This Colab notebook demonstrates what is necessary to install the necessary Python dependencies and expose a command-line endpoint for the submission jar file to communicate with.\n",
        "\n",
        "## Steps to run RAIPerformanceTournament"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uPHqOiXBTIqe"
      },
      "source": [
        "1. Download the `rl_algo_impls-0.2.1.zip` submission file and unzip to reveal `RAISocketAI.jar` and `rl_algo_impls-0.2.1-py3-none-any.whl` Python installation file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U69wxfaJS7WY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install -y subversion\n",
        "!wget https://github.com/sgoodfriend/rl-algo-impls/releases/download/v0.2.1/rl_algo_impls-0.2.1.zip\n",
        "!unzip -j rl_algo_impls-0.2.1.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DjVtuj6jmHB-"
      },
      "source": [
        "2. Install dependencies and rl_algo_impls Python wheel. This exposes the `rai_microrts` command-line endpoint, which will be called by the jar file. Ubuntu 20.04's default JDK is Java 11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-3xbBll4qJq"
      },
      "outputs": [],
      "source": [
        "# Fine if it prints errors as long as last line is \"Successfully installed\" with a long line of packages\n",
        "!apt install -y default-jdk\n",
        "!apt-get install swig\n",
        "!python -m pip install --upgrade pip\n",
        "!python -m pip install --upgrade torch\n",
        "!python -m pip install --upgrade rl_algo_impls-0.2.1-py3-none-any.whl"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TklKx6dRRCy4"
      },
      "source": [
        "3. Copy the jar files (microrts.jar and bot jars) and maps to run the tournament:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrGoTQ1QPJgw"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "\n",
        "# Get the package path\n",
        "PACKAGE_PATH=$(pip show rl_algo_impls | grep '^Location:' | awk '{print $2 \"/rl_algo_impls\"}')\n",
        "echo \"The package is located at: $PACKAGE_PATH\"\n",
        "\n",
        "# Define the source directories\n",
        "LIB_SOURCE=\"$PACKAGE_PATH/microrts/java/lib\"\n",
        "MAPS_SOURCE=\"$PACKAGE_PATH/microrts/java/maps\"\n",
        "\n",
        "# Define the target directories (current directory)\n",
        "LIB_TARGET=\"./lib\"\n",
        "MAPS_TARGET=\"./maps\"\n",
        "\n",
        "# Remove existing directories if they exist\n",
        "rm -rf \"$LIB_TARGET\" \"$MAPS_TARGET\"\n",
        "\n",
        "# Copy directories\n",
        "cp -r \"$LIB_SOURCE\" \"$LIB_TARGET\"\n",
        "cp -r \"$MAPS_SOURCE\" \"$MAPS_TARGET\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "koaWotNuu-dx"
      },
      "source": [
        "4. Run `tournaments.RAIPerformanceTournament` which runs 1 iteration of `RAIBCPPO`\n",
        "   against 2 opponents on 3 maps of different sizes (16x16, 32x32, and 64x64). Depending on hardware, `RAIBCPPO` can average from 50ms+\n",
        "   per turn on a Colab standard machine with 2-2.2GHz CPUs to 10-30ms per turn on modern\n",
        "   CPUs, such as an Apple M1Max."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DoCNZ6f6oFC"
      },
      "outputs": [],
      "source": [
        "!java -cp \"RAISocketAI.jar:$(find lib -name \"*.jar\" | tr '\\n' ':')\" tournaments.RAIPerformanceTournament -m RAI-BC-PPO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J7-2ZDbdnYyq"
      },
      "source": [
        "## Performance Discussion for RAISocketAI (not RAIBCPPO)\n",
        "This Colab notebook demonstrates that a GPU isn't required to run this agent. *However, if a GPU isn't available, a relatively beefy CPU is instead required for a model of this size to reliably compete under the 100ms turn timelimit.* The model has 32 convolutional layers and about 5 million parameters. So, beefy, but still smaller than something like a resnet50.\n",
        "\n",
        "PyTorch uses CPU multithreading to speedup the inference, so CPU clock speed and available cores are both helpful. For example, here's the performance on hardware I've tested so far:\n",
        "\n",
        "| CPU          | Threads | Clock Speed | 16x16 avg turn | 64x64 avg turn |\n",
        "| -------------| --------| ------------| ---------------| ---------------|\n",
        "| Colab        | 2       | 2.2 GHz     | 30-60 ms       | 450 ms         |\n",
        "| Apple M1 Max | 8+2     | 3.2/2.2 GHz | 16 ms          | 60-65 ms       |\n",
        "| Xeon 8358    | 30*     | 2.6 GHz     | 11 ms          | 25 ms          |\n",
        "\n",
        "*CPU multithreading is capped to 16 threads in PyTorch\n",
        "\n",
        "Occassionally, the agent will exceed 100 ms, even if the average is significantly below 100 ms. This happens more if there's other activity happening on the machine (for example, me using my own computer).\n",
        "\n",
        "If the long computations are rare (less than once per game), a timeout on the Java side to submit an empty turn if the Python server doesn't respond within 100 ms is a good safety valve, though it could mean that some CPU cores are taken up by the Python server on the opponent's turn (killing the Python process instead would lead to considerable delay starting the server and loading the PyTorch model).\n",
        "\n",
        "***However, if the competition machines are closer to the Colab machines, then a significantly smaller model is required.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPvFqG05qWdECt7CxjZZEzc",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
