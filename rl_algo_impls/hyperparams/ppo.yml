CartPole-v1: &cartpole-defaults
  n_timesteps: !!float 1e5
  env_hyperparams:
    n_envs: 8
  algo_hyperparams:
    batch_size: 256
    n_epochs: 20
    gae_lambda: 0.8
    gamma: 0.98
    ent_coef: 0.0
    learning_rate: 0.001
    learning_rate_decay: linear
    clip_range: 0.2
    clip_range_decay: linear
  eval_hyperparams:
    step_freq: !!float 2.5e4
  rollout_hyperparams:
    n_steps: 32

CartPole-v0:
  <<: *cartpole-defaults
  n_timesteps: !!float 5e4

MountainCar-v0:
  n_timesteps: !!float 1e6
  env_hyperparams:
    normalize: true
    n_envs: 16
  rollout_hyperparams:
    n_steps: 16
  algo_hyperparams:
    n_epochs: 4
    gae_lambda: 0.98
    gamma: 0.99
    ent_coef: 0.0

MountainCarContinuous-v0:
  n_timesteps: !!float 1e5
  env_hyperparams:
    normalize: true
    n_envs: 4
  # policy_hyperparams:
  #   init_layers_orthogonal: false
  #   log_std_init: -3.29
  #   use_sde: true
  rollout_hyperparams:
    n_steps: 512
  algo_hyperparams:
    batch_size: 256
    n_epochs: 10
    learning_rate: !!float 7.77e-5
    ent_coef: 0.01 # 0.00429
    ent_coef_decay: linear
    clip_range: 0.1
    gae_lambda: 0.9
    max_grad_norm: 5
    vf_coef: 0.19
  eval_hyperparams:
    step_freq: 5000

Acrobot-v1:
  n_timesteps: !!float 1e6
  env_hyperparams:
    n_envs: 16
    normalize: true
  rollout_hyperparams:
    n_steps: 256
  algo_hyperparams:
    n_epochs: 4
    gae_lambda: 0.94
    gamma: 0.99
    ent_coef: 0.0

LunarLander-v2:
  n_timesteps: !!float 4e6
  env_hyperparams:
    n_envs: 16
  rollout_hyperparams:
    n_steps: 1024
  algo_hyperparams:
    batch_size: 64
    n_epochs: 4
    gae_lambda: 0.98
    gamma: 0.999
    learning_rate: !!float 5e-4
    learning_rate_decay: linear
    clip_range: 0.2
    clip_range_decay: linear
    ent_coef: 0.01
    normalize_advantage: false

BipedalWalker-v3:
  n_timesteps: !!float 10e6
  env_hyperparams:
    n_envs: 16
    normalize: true
  rollout_hyperparams:
    n_steps: 2048
  algo_hyperparams:
    batch_size: 64
    gae_lambda: 0.95
    gamma: 0.99
    n_epochs: 10
    ent_coef: 0.001
    learning_rate: !!float 2.5e-4
    learning_rate_decay: linear
    clip_range: 0.2
    clip_range_decay: linear

CarRacing-v0: &carracing-defaults
  n_timesteps: !!float 4e6
  env_hyperparams:
    n_envs: 8
    frame_stack: 4
  policy_hyperparams: &carracing-policy-defaults
    use_sde: true
    log_std_init: -2
    init_layers_orthogonal: false
    activation_fn: relu
    share_features_extractor: false
    cnn_flatten_dim: 256
    hidden_sizes: [256]
  rollout_hyperparams:
    n_steps: 512
    sde_sample_freq: 4
  algo_hyperparams:
    batch_size: 128
    n_epochs: 10
    learning_rate: !!float 1e-4
    learning_rate_decay: linear
    gamma: 0.99
    gae_lambda: 0.95
    ent_coef: 0.0
    max_grad_norm: 0.5
    vf_coef: 0.5
    clip_range: 0.2

impala-CarRacing-v0:
  <<: *carracing-defaults
  env_id: CarRacing-v0
  policy_hyperparams:
    <<: *carracing-policy-defaults
    cnn_style: impala
    init_layers_orthogonal: true
    cnn_layers_init_orthogonal: false
    hidden_sizes: []

# BreakoutNoFrameskip-v4
# PongNoFrameskip-v4
# SpaceInvadersNoFrameskip-v4
# QbertNoFrameskip-v4
_atari: &atari-defaults
  n_timesteps: !!float 1e7
  env_hyperparams: &atari-env-defaults
    n_envs: 8
    frame_stack: 4
    no_reward_timeout_steps: 1000
    no_reward_fire_steps: 500
    vec_env_class: async
  policy_hyperparams: &atari-policy-defaults
    activation_fn: relu
  rollout_hyperparams: &atari-rollout-defaults
    n_steps: 128
  algo_hyperparams: &atari-algo-defaults
    batch_size: 256
    n_epochs: 4
    learning_rate: !!float 2.5e-4
    learning_rate_decay: linear
    clip_range: 0.1
    clip_range_decay: linear
    vf_coef: 0.5
    ent_coef: 0.01
  eval_hyperparams:
    deterministic: false

_norm-rewards-atari: &norm-rewards-atari-default
  <<: *atari-defaults
  env_hyperparams:
    <<: *atari-env-defaults
    clip_atari_rewards: false
    normalize: true
    normalize_kwargs:
      norm_obs: false
      norm_reward: true

norm-rewards-BreakoutNoFrameskip-v4:
  <<: *norm-rewards-atari-default
  env_id: BreakoutNoFrameskip-v4

debug-PongNoFrameskip-v4:
  <<: *atari-defaults
  device: cpu
  env_id: PongNoFrameskip-v4
  env_hyperparams:
    <<: *atari-env-defaults
    vec_env_class: sync

_impala-atari: &impala-atari-defaults
  <<: *atari-defaults
  policy_hyperparams:
    <<: *atari-policy-defaults
    cnn_style: impala
    cnn_flatten_dim: 256
    init_layers_orthogonal: true
    cnn_layers_init_orthogonal: false

impala-PongNoFrameskip-v4:
  <<: *impala-atari-defaults
  env_id: PongNoFrameskip-v4

impala-BreakoutNoFrameskip-v4:
  <<: *impala-atari-defaults
  env_id: BreakoutNoFrameskip-v4

impala-SpaceInvadersNoFrameskip-v4:
  <<: *impala-atari-defaults
  env_id: SpaceInvadersNoFrameskip-v4

impala-QbertNoFrameskip-v4:
  <<: *impala-atari-defaults
  env_id: QbertNoFrameskip-v4

_microrts: &microrts-defaults
  <<: *atari-defaults
  n_timesteps: !!float 2e6
  env_hyperparams: &microrts-env-defaults
    n_envs: 8
    vec_env_class: sync
    mask_actions: true
  policy_hyperparams: &microrts-policy-defaults
    <<: *atari-policy-defaults
    cnn_style: microrts
    cnn_flatten_dim: 128
  algo_hyperparams: &microrts-algo-defaults
    <<: *atari-algo-defaults
    clip_range_decay: none
    clip_range_vf: 0.1
    ppo2_vf_coef_halving: true
  eval_hyperparams: &microrts-eval-defaults
    deterministic: false # Good idea because MultiCategorical mode isn't great

_no-mask-microrts: &no-mask-microrts-defaults
  <<: *microrts-defaults
  env_hyperparams:
    <<: *microrts-env-defaults
    mask_actions: false

MicrortsMining-v1-NoMask:
  <<: *no-mask-microrts-defaults
  env_id: MicrortsMining-v1

MicrortsAttackShapedReward-v1-NoMask:
  <<: *no-mask-microrts-defaults
  env_id: MicrortsAttackShapedReward-v1

MicrortsRandomEnemyShapedReward3-v1-NoMask:
  <<: *no-mask-microrts-defaults
  env_id: MicrortsRandomEnemyShapedReward3-v1

_microrts_ai: &microrts-ai-defaults
  <<: *microrts-defaults
  n_timesteps: !!float 100e6
  additional_keys_to_log: ["microrts_stats", "microrts_results", "results"]
  env_hyperparams: &microrts-ai-env-defaults
    n_envs: 24
    env_type: microrts
    make_kwargs: &microrts-ai-env-make-kwargs-defaults
      num_selfplay_envs: 0
      max_steps: 4000
      render_theme: 2
      map_paths: [maps/16x16/basesWorkers16x16.xml]
      reward_weight: [10.0, 1.0, 1.0, 0.2, 1.0, 4.0, 0]
  policy_hyperparams: &microrts-ai-policy-defaults
    <<: *microrts-policy-defaults
    cnn_flatten_dim: 256
    actor_head_style: gridnet
  rollout_hyperparams: &microrts-ai-rollout-defaults
    n_steps: 512
  algo_hyperparams: &microrts-ai-algo-defaults
    <<: *microrts-algo-defaults
    learning_rate: !!float 2.5e-4
    learning_rate_decay: linear
    batch_size: 3072
    n_epochs: 4
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    clip_range: 0.1
    clip_range_vf: 0.1
  eval_hyperparams: &microrts-ai-eval-defaults
    <<: *microrts-eval-defaults
    score_function: mean
    max_video_length: 4000
    env_overrides: &microrts-ai-eval-env-overrides
      make_kwargs: &microrts-ai-eval-env-make-kwargs-overrides
        <<: *microrts-ai-env-make-kwargs-defaults
        max_steps: 4000
        reward_weight: [1.0, 0, 0, 0, 0, 0, 0]

MicrortsAttackPassiveEnemySparseReward-v3:
  <<: *microrts-ai-defaults
  n_timesteps: !!float 2e6
  env_id: MicrortsAttackPassiveEnemySparseReward-v3 # Workaround to keep model name simple
  env_hyperparams:
    <<: *microrts-ai-env-defaults
    bots:
      passiveAI: 24

MicrortsDefeatRandomEnemySparseReward-v3: &microrts-random-ai-defaults
  <<: *microrts-ai-defaults
  n_timesteps: !!float 2e6
  env_id: MicrortsDefeatRandomEnemySparseReward-v3 # Workaround to keep model name simple
  env_hyperparams:
    <<: *microrts-ai-env-defaults
    bots:
      randomBiasedAI: 24

enc-dec-MicrortsDefeatRandomEnemySparseReward-v3:
  <<: *microrts-random-ai-defaults
  policy_hyperparams:
    <<: *microrts-ai-policy-defaults
    cnn_style: gridnet_encoder
    actor_head_style: gridnet_decoder
    v_hidden_sizes: [128]

unet-MicrortsDefeatRandomEnemySparseReward-v3:
  <<: *microrts-random-ai-defaults
  # device: cpu
  policy_hyperparams:
    <<: *microrts-ai-policy-defaults
    actor_head_style: unet
    v_hidden_sizes: [256, 128]
  algo_hyperparams:
    <<: *microrts-ai-algo-defaults
    learning_rate: !!float 2.5e-4
    learning_rate_decay: spike

MicrortsDefeatCoacAIShaped-v3: &microrts-coacai-defaults
  <<: *microrts-ai-defaults
  env_id: MicrortsDefeatCoacAIShaped-v3 # Workaround to keep model name simple
  n_timesteps: !!float 300e6
  env_hyperparams: &microrts-coacai-env-defaults
    <<: *microrts-ai-env-defaults
    bots:
      coacAI: 24
  eval_hyperparams: &microrts-coacai-eval-defaults
    <<: *microrts-ai-eval-defaults
    step_freq: !!float 1e6
    n_episodes: 28
    env_overrides: &microrts-coacai-eval-env-overrides
      <<: *microrts-ai-eval-env-overrides
      n_envs: 28
      bots:
        coacAI: 2
        randomBiasedAI: 2
        randomAI: 2
        passiveAI: 2
        workerRushAI: 2
        lightRushAI: 2
        naiveMCTSAI: 2
        mixedBot: 2
        rojo: 2
        izanagi: 2
        tiamat: 2
        droplet: 2
        guidedRojoA3N: 2
        mayari: 2

MicrortsDefeatCoacAIShaped-v3-diverseBots: &microrts-diverse-defaults
  <<: *microrts-coacai-defaults
  env_hyperparams:
    <<: *microrts-coacai-env-defaults
    bots:
      coacAI: 18
      randomBiasedAI: 2
      lightRushAI: 2
      workerRushAI: 2

enc-dec-MicrortsDefeatCoacAIShaped-v3-diverseBots:
  &microrts-env-dec-diverse-defaults
  <<: *microrts-diverse-defaults
  policy_hyperparams:
    <<: *microrts-ai-policy-defaults
    cnn_style: gridnet_encoder
    actor_head_style: gridnet_decoder
    v_hidden_sizes: [128]

debug-enc-dec-MicrortsDefeatCoacAIShaped-v3-diverseBots:
  <<: *microrts-env-dec-diverse-defaults
  n_timesteps: !!float 1e6

unet-MicrortsDefeatCoacAIShaped-v3-diverseBots: &microrts-unet-defaults
  <<: *microrts-diverse-defaults
  policy_hyperparams:
    <<: *microrts-ai-policy-defaults
    actor_head_style: unet
    v_hidden_sizes: [256, 128]
  algo_hyperparams: &microrts-unet-algo-defaults
    <<: *microrts-ai-algo-defaults
    learning_rate: !!float 2.5e-4
    learning_rate_decay: spike

Microrts-selfplay-unet: &microrts-selfplay-defaults
  <<: *microrts-unet-defaults
  env_hyperparams: &microrts-selfplay-env-defaults
    <<: *microrts-ai-env-defaults
    make_kwargs: &microrts-selfplay-env-make-kwargs-defaults
      <<: *microrts-ai-env-make-kwargs-defaults
      num_selfplay_envs: 36
    self_play_kwargs: &microrts-selfplay-self-play-kwargs-defaults
      num_old_policies: 12
      save_steps: 300000
      swap_steps: 6000
      swap_window_size: 4
      window: 33
  eval_hyperparams: &microrts-selfplay-eval-defaults
    <<: *microrts-coacai-eval-defaults
    env_overrides: &microrts-selfplay-eval-env-overrides
      <<: *microrts-coacai-eval-env-overrides
      self_play_kwargs: {}

Microrts-selfplay-unet-winloss: &microrts-selfplay-winloss-defaults
  <<: *microrts-selfplay-defaults
  env_hyperparams:
    <<: *microrts-selfplay-env-defaults
    make_kwargs:
      <<: *microrts-selfplay-env-make-kwargs-defaults
      reward_weight: [1.0, 0, 0, 0, 0, 0, 0]
  algo_hyperparams: &microrts-selfplay-winloss-algo-defaults
    <<: *microrts-unet-algo-defaults
    gamma: 0.999

Microrts-selfplay-unet-decay: &microrts-selfplay-decay-defaults
  <<: *microrts-selfplay-defaults
  reward_decay_callback: true
  algo_hyperparams:
    <<: *microrts-unet-algo-defaults
    gamma_end: 0.999

Microrts-selfplay-unet-debug: &microrts-selfplay-debug-defaults
  <<: *microrts-selfplay-decay-defaults
  eval_hyperparams:
    <<: *microrts-selfplay-eval-defaults
    step_freq: !!float 1e5
    env_overrides:
      <<: *microrts-selfplay-eval-env-overrides
      n_envs: 24
      bots:
        coacAI: 12
        randomBiasedAI: 4
        workerRushAI: 4
        lightRushAI: 4

Microrts-selfplay-unet-debug-mps:
  <<: *microrts-selfplay-debug-defaults
  device: mps

Microrts-selfplay-dc-phases: &microrts-sp-dc-phases-defaults
  <<: *microrts-selfplay-defaults
  env_hyperparams: &microrts-sp-dc-phases-env-defaults
    <<: *microrts-selfplay-env-defaults
    additional_win_loss_reward: true
    score_reward_kwargs:
      delta_every_step: true
    map_paths:
      - maps/16x16/basesWorkers16x16A.xml
      - maps/16x16/TwoBasesBarracks16x16.xml
      - maps/8x8/basesWorkers8x8A.xml
      - maps/8x8/FourBasesWorkers8x8.xml
      - maps/NoWhereToRun9x8.xml
      - maps/16x16/EightBasesWorkers16x16.xml # Not public competition map
      # - maps/DoubleGame24x24.xml # 24x24
      # - maps/BWDistantResources32x32.xml # 32x32
      # - maps/BroodWar/(4)BloodBath.scmB.xml # 64x64
  eval_hyperparams: &microrts-sp-dc-phases-eval-defaults
    <<: *microrts-selfplay-eval-defaults
    env_overrides: &microrts-sp-dc-phases-eval-env-overrides
      <<: *microrts-selfplay-eval-env-overrides
      additional_win_loss_reward: false
      score_reward_kwargs: {}
      map_paths: [] # Use default map instead
  policy_hyperparams: &microrts-sp-dc-phases-policy-defaults
    <<: *microrts-ai-policy-defaults
    actor_head_style: double_cone
    pooled_channels: 128
    additional_critic_activation_functions: [tanh, identity]
  algo_hyperparams: &microrts-sp-dc-phases-algo-defaults
    <<: *microrts-ai-algo-defaults
    learning_rate_decay: none
    n_epochs: 2
    gamma: [0.99, 0.999, 0.999]
    gae_lambda: [0.95, 0.99, 0.99]
  hyperparam_transitions_kwargs: &microrts-sp-dc-phases-transitions-defaults
    phases:
      - multi_reward_weights: [0.8, 0.01, 0.19] # Initial shaped rewards (mostly)
        vf_coef: [0.5, 0.1, 0.2]
        ent_coef: 0.01
        learning_rate: !!float 1e-4
      - multi_reward_weights: [0, 0.5, 0.5] # Win-loss + Score delta
        vf_coef: [0, 0.4, 0.4]
        ent_coef: 0.01
        learning_rate: !!float 6e-4
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.5, 0.1]
        ent_coef: 0.001
        learning_rate: !!float 3e-5
    durations:
      - 0.35
      - 0.2
      - 0.05
      - 0.2
      - 0.2

Microrts-selfplay-dc-phases-A10: &microrts-sp-dc-phases-a10-defaults
  <<: *microrts-sp-dc-phases-defaults
  algo_hyperparams: &microrts-sp-dc-phases-a10-algo-defaults
    <<: *microrts-sp-dc-phases-algo-defaults
    batch_size: 2048

Microrts-selfplay-dc-phases-A6000: &microrts-sp-dc-phases-a6000-defaults
  <<: *microrts-sp-dc-phases-defaults
  algo_hyperparams: &microrts-sp-dc-phases-a10-algo-defaults
    <<: *microrts-sp-dc-phases-algo-defaults
    batch_size: 4096

_Microrts-A10-tweaks-base: &microrts-a10-tweaks-base
  <<: *microrts-sp-dc-phases-a10-defaults
  policy_hyperparams:
    <<: *microrts-sp-dc-phases-policy-defaults
    load_run_path: sgoodfriend/rl-algo-impls-benchmarks/nhweti29
    load_run_path_best: true

Microrts-A10-finetuned: &microrts-a10-finetuned
  <<: *microrts-a10-tweaks-base
  n_timesteps: !!float 100e6
  hyperparam_transitions_kwargs:
    phases:
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.5, 0.1]
        ent_coef: 0.01
        learning_rate: !!float 5e-5
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.5, 0.1]
        ent_coef: 0.01
        learning_rate: !!float 1e-4
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.5, 0.1]
        ent_coef: 0.001
        learning_rate: !!float 5e-5
    durations:
      - 0
      - 0.1
      - 0.25
      - 0.2
      - 0.45

Microrts-A6000-finetuned-shaped: &microrts-a6000-finetuned-shaped
  <<: *microrts-a10-tweaks-base
  n_timesteps: !!float 100e6
  algo_hyperparams:
    <<: *microrts-sp-dc-phases-a10-algo-defaults
    batch_size: 4096
    gamma: [0.99, 0.999, 0.999]
    gae_lambda: [0.95, 0.99, 0.99]
  hyperparam_transitions_kwargs:
    phases:
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.4, 0.2]
        ent_coef: 0.01
        learning_rate: !!float 1e-5
      - multi_reward_weights: [0, 0.5, 0.5] # Win-loss + Score delta
        vf_coef: [0, 0.4, 0.4]
        ent_coef: 0.01
        learning_rate: !!float 5e-5
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.5, 0.1]
        ent_coef: 0.001
        learning_rate: !!float 5e-5
    durations:
      - 0
      - 0.05
      - 0.3
      - 0.2
      - 0.45

Microrts-A6000-finetuned-coac: &microrts-a6000-finetuned-coac
  <<: *microrts-a6000-finetuned-shaped
  env_hyperparams: &microrts-a6000-finetuned-coac-env-defaults
    <<: *microrts-sp-dc-phases-env-defaults
    make_kwargs: &microrts-a6000-finetuned-coac-env-make-kwargs
      <<: *microrts-selfplay-env-make-kwargs-defaults
      num_selfplay_envs: 12
    self_play_kwargs:
      <<: *microrts-selfplay-self-play-kwargs-defaults
      num_old_policies: 0
    bots:
      coacAI: 12

Microrts-A6000-finetuned-coac-mayari: &microrts-a6000-finetuned-coac-mayari
  <<: *microrts-a6000-finetuned-coac
  env_hyperparams:
    <<: *microrts-a6000-finetuned-coac-env-defaults
    make_kwargs:
      <<: *microrts-a6000-finetuned-coac-env-make-kwargs
      bot_envs_alternate_player: true
    bots:
      coacAI: 6
      mayari: 6
  eval_hyperparams:
    <<: *microrts-sp-dc-phases-eval-defaults
    env_overrides:
      <<: *microrts-sp-dc-phases-eval-env-overrides
      make_kwargs:
        <<: *microrts-ai-eval-env-make-kwargs-overrides
        bot_envs_alternate_player: false

Microrts-selfplay-dc-phases-maps24: &microrts-sp-dc-phases-m24-defaults
  <<: *microrts-sp-dc-phases-defaults
  env_hyperparams:
    <<: *microrts-sp-dc-phases-env-defaults
    map_paths:
      - maps/16x16/basesWorkers16x16A.xml
      - maps/16x16/TwoBasesBarracks16x16.xml
      - maps/8x8/basesWorkers8x8A.xml
      - maps/8x8/FourBasesWorkers8x8.xml
      - maps/NoWhereToRun9x8.xml
      - maps/DoubleGame24x24.xml # 24x24
      # - maps/BWDistantResources32x32.xml # 32x32
      # - maps/BroodWar/(4)BloodBath.scmB.xml # 64x64
  algo_hyperparams:
    <<: *microrts-sp-dc-phases-algo-defaults
    batch_size: 1536

Microrts-selfplay-dc-phases-final: &microrts-sp-dc-phases-final-defaults
  <<: *microrts-sp-dc-phases-defaults
  n_timesteps: !!float 100e6
  env_hyperparams:
    <<: *microrts-sp-dc-phases-env-defaults
    n_envs: 32
    make_kwargs:
      <<: *microrts-selfplay-env-make-kwargs-defaults
      num_selfplay_envs: 48
      max_steps: 8000
    self_play_kwargs:
      <<: *microrts-selfplay-self-play-kwargs-defaults
      num_old_policies: 16
      save_steps: 600000
      window: 66 # 40M steps window
      swap_steps: 10000
    map_paths:
      - maps/16x16/basesWorkers16x16A.xml
      - maps/16x16/TwoBasesBarracks16x16.xml
      - maps/8x8/basesWorkers8x8A.xml
      - maps/8x8/FourBasesWorkers8x8.xml
      - maps/NoWhereToRun9x8.xml
      - maps/DoubleGame24x24.xml # 24x24
      - maps/BWDistantResources32x32.xml # 32x32
      - maps/BroodWar/(4)BloodBath.scmB.xml # 64x64
  rollout_hyperparams:
    <<: *microrts-ai-rollout-defaults
    n_steps: 128
  algo_hyperparams:
    <<: *microrts-sp-dc-phases-algo-defaults
    batch_size: 128
  hyperparam_transitions_kwargs:
    phases:
      - multi_reward_weights: [0, 0.67, 0.33] # Win-loss sparse rewards
        vf_coef: [0, 0.4, 0.3]
        ent_coef: 0.01
        learning_rate: !!float 1e-4
      - multi_reward_weights: [0, 0.99, 0.01] # Win-loss sparse rewards
        vf_coef: [0, 0.5, 0.1]
        ent_coef: 0.001
        learning_rate: !!float 5e-5
    durations:
      - 0.5
      - 0.4
      - 0.1

Microrts-selfplay-dc-phases-debug: &microrts-sp-dc-phases-debug-defaults
  <<: *microrts-a6000-finetuned-coac-mayari
  eval_hyperparams:
    <<: *microrts-sp-dc-phases-eval-defaults
    step_freq: !!float 1e5

Microrts-agent: &microrts-agent
  <<: *microrts-sp-dc-phases-defaults
  device: cpu
  eval_hyperparams:
    <<: *microrts-sp-dc-phases-eval-defaults
    env_overrides:
      <<: *microrts-sp-dc-phases-eval-env-overrides
      n_envs: 1
      self_play_kwargs:
        save_steps: .inf
        swap_steps: .inf
      is_agent: true

HalfCheetahBulletEnv-v0: &pybullet-defaults
  n_timesteps: !!float 2e6
  env_hyperparams: &pybullet-env-defaults
    n_envs: 16
    normalize: true
  policy_hyperparams: &pybullet-policy-defaults
    pi_hidden_sizes: [256, 256]
    v_hidden_sizes: [256, 256]
    activation_fn: relu
  rollout_hyperparams: &pybullet-rollout-defaults
    n_steps: 512
  algo_hyperparams: &pybullet-algo-defaults
    batch_size: 128
    n_epochs: 20
    gamma: 0.99
    gae_lambda: 0.9
    ent_coef: 0.0
    max_grad_norm: 0.5
    vf_coef: 0.5
    learning_rate: !!float 3e-5
    clip_range: 0.4

AntBulletEnv-v0:
  <<: *pybullet-defaults
  policy_hyperparams:
    <<: *pybullet-policy-defaults
  algo_hyperparams:
    <<: *pybullet-algo-defaults

Walker2DBulletEnv-v0:
  <<: *pybullet-defaults
  algo_hyperparams:
    <<: *pybullet-algo-defaults
    clip_range_decay: linear

HopperBulletEnv-v0:
  <<: *pybullet-defaults
  algo_hyperparams:
    <<: *pybullet-algo-defaults
    clip_range_decay: linear

HumanoidBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 1e7
  env_hyperparams:
    <<: *pybullet-env-defaults
    n_envs: 8
  policy_hyperparams:
    <<: *pybullet-policy-defaults
    # log_std_init: -1
  rollout_hyperparams:
    <<: *pybullet-rollout-defaults
    n_steps: 2048
  algo_hyperparams:
    <<: *pybullet-algo-defaults
    batch_size: 64
    n_epochs: 10
    gae_lambda: 0.95
    learning_rate: !!float 2.5e-4
    clip_range: 0.2

_procgen: &procgen-defaults
  env_hyperparams: &procgen-env-defaults
    env_type: procgen
    n_envs: 64
    # grayscale: false
    # frame_stack: 4
    normalize: true # procgen only normalizes reward
    make_kwargs: &procgen-make-kwargs-defaults
      num_threads: 8
  policy_hyperparams: &procgen-policy-defaults
    activation_fn: relu
    cnn_style: impala
    cnn_flatten_dim: 256
    init_layers_orthogonal: true
    cnn_layers_init_orthogonal: false
  rollout_hyperparams: &procgen-rollout-defaults
    n_steps: 256
  algo_hyperparams: &procgen-algo-defaults
    gamma: 0.999
    gae_lambda: 0.95
    batch_size: 2048
    n_epochs: 3
    ent_coef: 0.01
    clip_range: 0.2
    # clip_range_decay: linear
    clip_range_vf: 0.2
    learning_rate: !!float 5e-4
    # learning_rate_decay: linear
    vf_coef: 0.5
  eval_hyperparams: &procgen-eval-defaults
    ignore_first_episode: true
    # deterministic: false
    step_freq: !!float 1e5

_procgen-easy: &procgen-easy-defaults
  <<: *procgen-defaults
  n_timesteps: !!float 25e6
  env_hyperparams: &procgen-easy-env-defaults
    <<: *procgen-env-defaults
    make_kwargs:
      <<: *procgen-make-kwargs-defaults
      distribution_mode: easy

procgen-coinrun-easy: &coinrun-easy-defaults
  <<: *procgen-easy-defaults
  env_id: coinrun

debug-procgen-coinrun:
  <<: *coinrun-easy-defaults
  device: cpu

procgen-starpilot-easy:
  <<: *procgen-easy-defaults
  env_id: starpilot

procgen-bossfight-easy:
  <<: *procgen-easy-defaults
  env_id: bossfight

procgen-bigfish-easy:
  <<: *procgen-easy-defaults
  env_id: bigfish

_procgen-hard: &procgen-hard-defaults
  <<: *procgen-defaults
  n_timesteps: !!float 200e6
  env_hyperparams: &procgen-hard-env-defaults
    <<: *procgen-env-defaults
    n_envs: 256
    make_kwargs:
      <<: *procgen-make-kwargs-defaults
      distribution_mode: hard
  algo_hyperparams: &procgen-hard-algo-defaults
    <<: *procgen-algo-defaults
    batch_size: 8192
    clip_range_decay: linear
    learning_rate_decay: linear
  eval_hyperparams:
    <<: *procgen-eval-defaults
    step_freq: !!float 5e5

procgen-starpilot-hard: &procgen-starpilot-hard-defaults
  <<: *procgen-hard-defaults
  env_id: starpilot

procgen-starpilot-hard-2xIMPALA:
  <<: *procgen-starpilot-hard-defaults
  policy_hyperparams:
    <<: *procgen-policy-defaults
    impala_channels: [32, 64, 64]
  algo_hyperparams:
    <<: *procgen-hard-algo-defaults
    learning_rate: !!float 3.3e-4

procgen-starpilot-hard-2xIMPALA-fat:
  <<: *procgen-starpilot-hard-defaults
  policy_hyperparams:
    <<: *procgen-policy-defaults
    impala_channels: [32, 64, 64]
    cnn_flatten_dim: 512
  algo_hyperparams:
    <<: *procgen-hard-algo-defaults
    learning_rate: !!float 2.5e-4

procgen-starpilot-hard-4xIMPALA:
  <<: *procgen-starpilot-hard-defaults
  policy_hyperparams:
    <<: *procgen-policy-defaults
    impala_channels: [64, 128, 128]
  algo_hyperparams:
    <<: *procgen-hard-algo-defaults
    learning_rate: !!float 2.1e-4

LuxAI_S2-v0: &lux-defaults
  n_timesteps: !!float 40e6
  additional_keys_to_log: ["stats", "results"]
  env_id: LuxAI_S2-v0
  env_hyperparams: &lux-env-defaults
    env_type: lux
    vec_env_class: sync
    n_envs: 24
    self_play_kwargs:
      num_old_policies: 12
      save_steps: 300000
      swap_steps: 3000
      swap_window_size: 4
      window: 33
    make_kwargs: &lux-env-make-defaults
      bid_std_dev: 0
      reward_weights:
        ice_generation: 0.0025 # 0.5/day of water for factory, 0.05/heavy dig
        ore_generation: 0.008 # 4/heavy robot build, 0.16/heavy dig
        water_generation: 0.01 # 0.5/day of water for factory
        metal_generation: 0.04 # 4/heavy robot build
        power_generation: 0
        ice_rubble_cleared: 0.002 # 80% of ice_generation
        ore_rubble_cleared: 0.0064 # 80% of ore_generation
        built_light: 0.1
        built_heavy: 1
        lost_factory: -1
        factories_alive: 0.01
        heavies_alive: 0.005 # 0.25/day of heavy alive
        lights_alive: 0.0005 # 0.025/day of light alive
    additional_win_loss_reward: true
    score_reward_kwargs:
      episode_end: true
  eval_hyperparams: &lux-eval-defaults
    deterministic: false
    step_freq: !!float 1e6
    score_function: mean
    only_record_video_on_best: true
    score_threshold: 0.27
    env_overrides: &lux-eval-env-overrides
      n_envs: 66
      self_play_kwargs:
        eval_use_training_cache: true
      make_kwargs: &lux-eval-env-override-make-kwargs
        <<: *lux-env-make-defaults
        reward_weights:
          win_loss: 1
      additional_win_loss_reward: false
      score_reward_kwargs: {}
  policy_hyperparams: &lux-policy-defaults
    activation_fn: relu
    actor_head_style: unet
    v_hidden_sizes: [256, 128]
    embed_layer: false
    additional_critic_activation_functions: [tanh, identity]
  hyperparam_transitions_kwargs: &lux-transitions-defaults
    phases:
      - multi_reward_weights: [0.98, 0.01, 0.01] # Initial shaped rewards (mostly)
        vf_coef: [0.5, 0.1, 0.1]
        ent_coef: 0.01
        learning_rate: !!float 1e-4
      - multi_reward_weights: [0.34, 0.33, 0.33] # Equal contributions
        vf_coef: [0.25, 0.25, 0.25]
        ent_coef: 0.01
        learning_rate: !!float 1e-4
      - multi_reward_weights: [0, 0.9, 0.1] # Win-loss sparse rewards
        vf_coef: [0, 0.4, 0.2]
        ent_coef: 0.001
        learning_rate: !!float 5e-5
    durations:
      - 0.3
      - 0.2
      - 0.2
      - 0.2
      - 0.1
  rollout_hyperparams: &lux-rollout-defaults
    n_steps: 384
  algo_hyperparams: &lux-algo-defaults
    batch_size: 2304
    n_epochs: 4
    learning_rate_decay: none
    ent_coef: 0.01
    max_grad_norm: 0.5
    clip_range: 0.1
    clip_range_decay: none
    clip_range_vf: 0.1
    ppo2_vf_coef_halving: true
    multi_reward_weights: [0.99, 0.01]
    gamma: [0.99, 0.999, 0.999]
    gae_lambda: [0.95, 0.99, 0.99]
    vf_coef: [0.5, 0.1, 0.1]

LuxAI_S2-v0-double-cone: &lux-dc-defaults
  <<: *lux-defaults
  policy_hyperparams:
    actor_head_style: double_cone
    pooled_channels: 128
    additional_critic_activation_functions: [tanh, identity]
  rollout_hyperparams: &lux-dc-rollout-defaults
    <<: *lux-rollout-defaults
    n_steps: 128
  algo_hyperparams: &lux-dc-algo-defaults
    <<: *lux-algo-defaults
    batch_size: 768
    n_epochs: 2

LuxAI_S2-v0-eval:
  <<: *lux-defaults
  device: cpu
  eval_hyperparams:
    <<: *lux-eval-defaults
    env_overrides:
      n_envs: 1
      self_play_kwargs:
        save_steps: .inf
        swap_steps: .inf
      make_kwargs:
        <<: *lux-env-make-defaults
        reward_weights:
          win_loss: 1

LuxAI_S2-v0-A10: &lux-a10-defaults
  <<: *lux-defaults
  rollout_hyperparams:
    n_steps: 192
  algo_hyperparams:
    <<: *lux-algo-defaults
    batch_size: 1152

LuxAI_S2-v0-A10-100M:
  <<: *lux-a10-defaults
  n_timesteps: !!float 100e6

LuxAI_S2-v0-dc-A10: &lux-dc-a10-defaults
  <<: *lux-dc-defaults
  rollout_hyperparams:
    <<: *lux-dc-rollout-defaults
    n_steps: 64
  algo_hyperparams:
    <<: *lux-dc-algo-defaults
    batch_size: 256

LuxAI_S2-v0-medium-transfer: &lux-medium-transfer-defaults
  <<: *lux-a10-defaults
  policy_hyperparams:
    <<: *lux-policy-defaults
    load_run_path: sgoodfriend/rl-algo-impls-benchmarks/qpm5k6n1
    load_run_path_best: false

LuxAI_S2-v0-medium-transfer-debug:
  <<: *lux-medium-transfer-defaults
  device: cpu
  env_hyperparams:
    <<: *lux-env-defaults
    vec_env_class: sync
    make_kwargs:
      <<: *lux-env-make-defaults
      verbose: 2
  hyperparam_transitions_kwargs:
    <<: *lux-transitions-defaults
    durations:
      - 0 # Should already know water generation, so transition to sparse rewards
      - 0.3
      - 0.2
      - 0.2
      - 0.3

LuxAI_S2-v0-medium-transfer-A100:
  <<: *lux-medium-transfer-defaults
  algo_hyperparams:
    <<: *lux-algo-defaults

LuxAI_S2-v0-small: &small-lux-defaults
  <<: *lux-defaults
  env_hyperparams: &small-lux-env-defaults
    <<: *lux-env-defaults
    make_kwargs:
      <<: *lux-env-make-defaults
      map_size: 16
      MIN_FACTORIES: 1
      MAX_FACTORIES: 1
  eval_hyperparams:
    <<: *lux-eval-defaults
    env_overrides:
      <<: *lux-eval-env-overrides
      make_kwargs:
        <<: *lux-eval-env-override-make-kwargs
        map_size: 16
        MIN_FACTORIES: 1
        MAX_FACTORIES: 1

LuxAI_S2-v0-small-debug:
  <<: *small-lux-defaults
  device: cpu
  env_hyperparams:
    <<: *small-lux-env-defaults
    vec_env_class: sync

LuxAI_S2-v0-medium: &medium-lux-defaults
  <<: *lux-defaults
  n_timesteps: !!float 30e6
  env_hyperparams: &medium-lux-env-defaults
    <<: *lux-env-defaults
    make_kwargs:
      <<: *lux-env-make-defaults
      map_size: 32
      MIN_FACTORIES: 1
      MAX_FACTORIES: 1
  eval_hyperparams:
    <<: *lux-eval-defaults
    env_overrides:
      <<: *lux-eval-env-overrides
      make_kwargs:
        <<: *lux-eval-env-override-make-kwargs
        map_size: 32
        MIN_FACTORIES: 1
        MAX_FACTORIES: 1

LuxAI_S2-v0-medium-debug:
  <<: *medium-lux-defaults
  device: cpu
  env_hyperparams:
    <<: *medium-lux-env-defaults
    vec_env_class: sync

LuxAI_S2-v0-debug:
  <<: *lux-dc-a10-defaults
  n_timesteps: !!float 1e6
  device: cpu
  env_hyperparams:
    <<: *lux-env-defaults
    vec_env_class: sync
    make_kwargs:
      <<: *lux-env-make-defaults
      verbose: 3
      verify: true
  eval_hyperparams:
    <<: *lux-eval-defaults
    step_freq: !!float 1e5
  hyperparam_transitions_kwargs:
    <<: *lux-transitions-defaults
    durations:
      - 0.05
      - 0.05
      - 0.05
      - 0.05
      - 0.8
